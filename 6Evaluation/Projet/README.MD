# Welcome to Amazon Web Scraper üëã

![Version](https://img.shields.io/badge/version-1.0-blue.svg?cacheSeconds=2592000)

> Un projet permettant de scrapper les produits Amazon, en recherchant un mot-cl√© ou un lien, pour suivre l‚Äô√©volution des prix et d√©tecter les opportunit√©s.

## üèóÔ∏è Architecture du projet

Le projet repose sur plusieurs composants interconnect√©s :

- **Flask (interface graphique)** : permet d'interagir avec l'application.
- **Scraping (scrapping_script.py)** : r√©cup√®re les donn√©es depuis Amazon.
- **MongoDB** : stocke les produits scrapp√©s.
- **Task-executer** : planifie le scrapping automatique.
- **Mongo-to-Elastic** : transfert les donn√©es de MongoDB vers Elasticsearch.
- **Elasticsearch** : moteur de recherche et d'analyse pour afficher les historiques de prix.
- **Kibana** : visualisation avanc√©e des donn√©es.

### üìå Sch√©ma global 
![](./interface/static/img/Diagramme.png)

---

## ‚öôÔ∏è Installation & Configuration

### 1Ô∏è‚É£ Pr√©requis

- **Docker & Docker Compose**
- **Python 3.9+**
- **Compte Amazon pour les tests de scrapping**

### 2Ô∏è‚É£ Installation

Clonez le projet :

```sh
git clone https://github.com/Alexadnre/DataEngineerTools.git

```

D√©marrez l'application :

```
cd DataEngineerTools/6Evaluation/Projet
docker-compose up --build -d

```

---

## üñ•Ô∏è Interface Utilisateur

L'interface graphique poss√®de **deux pages** :

### üìç 1. Recherche & Scrapping
- Recherche par **mot-cl√©** ou **URL**.
- S√©lection d'un **d√©lai d'ex√©cution automatique**.
- Ex√©cution du scrapping via **task-executer**.

![](./interface/static/img/screen_track.png)


### üìç 2. Consultation des donn√©es scrapp√©es
- Affichage des **produits stock√©s**.
- Graphique interactif de **l‚Äôhistorique des prix**.
- Auto-compl√©tion via **Elasticsearch**.


![](./interface/static/img/screen_suivi.png)

---

## üîç D√©tails Techniques

### üìå Scraping Amazon

**Probl√©matique** : Amazon, √©tant un site avec de nombreuses protections anti-scraping, peut bloquer les tentatives de scraping, notamment en raison des v√©rifications bas√©es sur les user-agents et des demandes trop fr√©quentes.

**Solution** :
Nous avons mis en place une s√©rie de techniques pour contourner ces protections et effectuer le scraping de mani√®re plus efficace et discr√®te :

1. **Utilisation d'un User-Agent Obsol√®te** :
   - Apr√®s plusieurs tests, nous avons d√©couvert qu‚Äôen utilisant un **User-Agent d'une vieille version d'Internet Explorer**, nous parvenions √† √©viter les blocages. Cette m√©thode a permis d‚Äôobtenir un acc√®s plus stable aux pages, car Amazon ne semble pas bloquer ce type de requ√™tes aussi facilement.
   
2. **D√©lai Al√©atoire entre les Requ√™tes** :
   - Nous avons ajout√© des **d√©lai al√©atoires** entre chaque requ√™te pour simuler un comportement plus naturel et √©viter de trop solliciter le serveur Amazon.
   
3. **Rotation des User-Agents** :
   - Pour augmenter la robustesse de la solution, nous avons mis en place une **rotation des User-Agents**. Nous avons utilis√© un ensemble de **User-Agents fonctionnels** trouv√©s durant nos tests pour √©viter que le m√™me user-agent soit utilis√© de mani√®re r√©p√©t√©e.

4. **Utilisation de Microsoft Edge pour l'Analyse HTML** :
   - Pour mieux comprendre la structure des pages d‚ÄôAmazon et savoir comment effectuer nos requ√™tes de scraping, nous avons utilis√© **Microsoft Edge** avec la fonctionnalit√© permettant de charger des pages au format **Internet Explorer**. Bien que cette approche soit **moins stable** et utilise beaucoup de m√©moire, elle nous a permis de mieux analyser l'HTML des pages et de d√©finir les meilleures requ√™tes pour extraire les informations dont nous avons besoin.
   
Cette m√©thode nous a permis de contourner efficacement les protections tout en maintenant une certaine stabilit√© et fiabilit√© dans notre processus de scraping.


### üìå Variables d'environement 
Toutes les variables d'envirnnement sont stock√©es dans un  **.env**, pour simplifier l'utilisation des conteneurs 

```python
# Nom des containers
MONGO_PRIMARY_CONTAINER=mongo1
MONGO_INIT_CONTAINER=mongo-init
ELASTICSEARCH_CONTAINER=elasticsearch-db
KIBANA_CONTAINER=kibana
MONGO_TRACKER_CONTAINER=mongo-tracker-py


# MongoDB

MONGO_HOST=mongo1
MONGO_PORT=27017
MONGO_REPLICA_SET=rs0
MONGO_DB=amazon_web_scraper
MONGO_DB_TABLE=products
MONGO_DB_TABLE_TRACKER=track_list

# Elasticsearch
ELASTIC_HOST=elasticsearch
ELASTIC_PORT=9200

# Kibana
KIBANA_PORT=5601

# Flask
FLASK_APP=app.py
FLASK_ENV=development



TASK_EXECUTER_CONTAINER=task-executer
```
### üìå MongoDB & Replica Set
Les donn√©es sont stock√©es dans **MongoDB**, configur√© en **Replica Set** pour assurer la synchronisation et la durabilit√©.

Extrait du `docker-compose.yml` :

```yaml
services:
  mongo1:
    image: mongo:6
    container_name: ${MONGO_PRIMARY_CONTAINER}
    command: ["mongod", "--replSet", "${MONGO_REPLICA_SET}", "--bind_ip_all"]
    ports:
      - "${MONGO_PORT}:${MONGO_PORT}"
    volumes:
      - mongo-data:/data/db
    networks:
      - mongo-cluster
```

### üìå Transfert Mongo ‚Üí Elastic
Les produits sont transf√©r√©s vers **Elasticsearch** gr√¢ce √† `mongo_to_elastic/tracker.py` en exploitant **MongoDB Replica Set**.

Extrait du code :

```python
pipeline = [
    {
        "$match": {
            "operationType": {"$in": ["insert", "update", "delete"]},  # Filtrer pour insert, update, delete
            "ns.coll": MONGO_DB_TABLE  # Filtrer pour la collection sp√©cifique
        }
    }
]   

# Surveiller les changements et les envoyer √† Elasticsearch
with db.watch(pipeline) #...
```

---

## üì° API & Requ√™tes

L‚Äôinterface utilise plusieurs **endpoints** :

| Endpoint                       | M√©thode | Description |
|---------------------------------|---------|-------------|
| `/scrape`                       | `POST`  | Lance le scrapping pour un mot-cl√© ou une URL d'Amazon. |
| `/products`                     | `GET`   | R√©cup√®re tous les produits stock√©s dans la base de donn√©es MongoDB. |
| `/history/<product_id>`         | `GET`   | Retourne l'historique des prix pour un produit sp√©cifique. |
| `/track`                        | `GET`, `POST` | Permet de suivre l'ajout de produits et de lancer des processus de suivi des prix. |
| `/delete_product`               | `POST`  | Permet de supprimer un produit de la liste de suivi par son `asin`. |
| `/check_connections`            | `GET`   | V√©rifie la connexion √† MongoDB et Elasticsearch. |
| `/autocomplete`                 | `GET`   | Fournit une auto-compl√©tion de produits en fonction de la requ√™te de recherche. |
| `/product/<product_id>`         | `GET`   | Retourne les d√©tails d'un produit √† partir de son `product_id` (ASIN). |

‚û°Ô∏è **Extrait JSON d'un produit scrapp√©.**
```json
{'name': "NiPoGi AM06 Pro Mini PC 32 Go DDR5 RAM, –êMD Ryzen 7 6800H(8C/16T, jusqu'√† 4,7 GHz), 512Go M.2 SSD Mini Ordinateur de Burea...", 'price': 399.99, 'asin': 'B0DT3DN71D', 'url': 'https://www.amazon.fr/dp/B0DT3DN71D', 'image_url': 'https://m.media-amazon.com/images/I/71NuJFbTBCL._AC_UL320_.jpg'}
```
---

## üõ†Ô∏è Tests & D√©veloppement

### Fichiers de test :

- `test/test_mongo_connection.py` : v√©rifie la connexion MongoDB.
- `test/test_mongo_request.py` : teste les requ√™tes Mongo.
- `test/test_update.py` : v√©rifie la mise √† jour des prix.

---

## üöÄ Am√©liorations Futures


### 1Ô∏è‚É£ Int√©gration de Notifications pour Alerter sur les Baisses de Prix üì©
**Objectif** : Ajouter un syst√®me de notification pour alerter les utilisateurs des baisses de prix des produits suivis.

- **Approche** : 
  - Impl√©menter un syst√®me de t√¢ches planifi√©es via **Flask** qui v√©rifiera p√©riodiquement les prix des produits dans la base de donn√©es.
  - Lorsqu'une baisse de prix est d√©tect√©e, une notification sera envoy√©e via un service comme **Twilio** (SMS) ou **SendGrid** (email).
  
- **Technologies** :
  - Utilisation de **Flask-Mail** pour l'envoi d'emails ou **Flask-SocketIO** pour les notifications en temps r√©el sur l'interface utilisateur.
  - Int√©gration d'un service tiers pour les notifications par SMS (par exemple, Twilio).

### 2Ô∏è‚É£ Am√©lioration du Moteur de Recherche via NLP ü§ñ
**Objectif** : Am√©liorer l'auto-compl√©tion et la recherche en utilisant des techniques de **traitement du langage naturel (NLP)**.

- **Approche** : 
  - Compl√©ter le moteur de recherche **Elasticsearch** avec des mod√®les **NLP** pour mieux comprendre et traiter les requ√™tes des utilisateurs.
  - Utiliser des biblioth√®ques telles que **spaCy** ou **Transformers** (par exemple, **BERT**) pour am√©liorer la pertinence des suggestions d'auto-compl√©tion et des r√©sultats de recherche.
  
- **Technologies** :
  - Int√©gration de **spaCy** pour l'analyse linguistique et la cat√©gorisation des termes de recherche.
  - Utilisation de mod√®les pr√©-entrain√©s comme **BERT** ou **DistilBERT** pour l'analyse s√©mantique des requ√™tes.

### 3Ô∏è‚É£ Am√©lioration de l'Interface Utilisateur üåê
**Objectif** : Refondre l'interface graphique pour am√©liorer l'exp√©rience utilisateur.

- **Approche** : 
  - Ajouter des √©l√©ments d'interface tels que des graphiques en temps r√©el des baisses de prix, une meilleure pr√©sentation des historiques de prix, et des alertes visuelles.
  - Introduire des **animations CSS** pour une exp√©rience plus fluide et interactive.
  - Ajouter des filtres avanc√©s pour trier les produits par cat√©gorie, prix, ou score moyen des avis.
  
- **Technologies** :
  - Utilisation de **JavaScript** et **React.js** pour une interface plus dynamique.
  - Ajouter un graphique interactif avec **Plotly** ou **D3.js** pour visualiser l'√©volution des prix.
  
### 4Ô∏è‚É£ Fonctionnalit√© de Suppression de Produit sur l'Interface üéØ
**Objectif** : Permettre √† l'utilisateur de supprimer facilement un produit de la liste de suivi via l'interface graphique.

- **Approche** : 
  - Ajouter un bouton "Supprimer" √† c√¥t√© de chaque produit dans la liste de suivi.
  - Lorsque l'utilisateur clique sur "Supprimer", la base de donn√©es sera mise √† jour pour retirer le produit, et l'interface sera r√©actualis√©e.
  
- **Technologies** :
  - Utilisation de **JavaScript** pour la mise √† jour dynamique de l'interface.
  - Int√©gration de l'API backend pour supprimer les produits dans la base de donn√©es MongoDB.

### 5Ô∏è‚É£ Am√©lioration de l'API pour Supporter plus de Formats de Donn√©es üîÑ
**Objectif** : √âtendre l'API pour accepter et renvoyer des donn√©es dans plus de formats, tels que CSV ou XML.

- **Approche** : 
  - Ajouter des options dans les requ√™tes API pour exporter les produits et leurs historiques sous forme de **fichiers CSV** ou **XML**.
  - Impl√©menter des routes API suppl√©mentaires pour la conversion et l'exportation de donn√©es.
  
- **Technologies** :
  - Utilisation de la biblioth√®que **pandas** pour g√©n√©rer les fichiers CSV.
  - Utilisation de **xml.etree.ElementTree** pour g√©rer les exportations au format XML.

### 6Ô∏è‚É£ Exemple de Scraping H√©berg√© sur AWS

Une version web du projet devait initialement √™tre h√©berg√©e sur **AWS** afin de fournir un exemple de scraping en continu. Cette version aurait permis de d√©montrer l'efficacit√© du scraping sur une longue p√©riode et de montrer les donn√©es en temps r√©el. Cependant, nous avons rencontr√© plusieurs d√©fis techniques :

- **Probl√®me avec Elasticsearch** : L'instance AWS **t2 du tiers gratuit** √©tait insuffisante pour supporter la charge d'**Elasticsearch**, qui est tr√®s gourmand en ressources. M√™me apr√®s avoir mis √† niveau l'instance vers une machine plus puissante et configur√© les ports correctement, nous n'avons pas r√©ussi √† charger **Flask** correctement, en raison d'un probl√®me d'ouverture des ports sur AWS.
  
- **Solution √† Long Terme** : Ce probl√®me peut √™tre r√©solu avec **plus de temps** pour ajuster la configuration du serveur, optimiser les performances d'Elasticsearch et tester diff√©rentes configurations de ressources.

---



## ‚ú® Auteurs

üë§ **Alexandre Videlaine & Parys Noyon**

- Github: [@Alexadnre](https://github.com/Alexadnre)
- Github: [@Parysnm](https://github.com/Parysnm)
