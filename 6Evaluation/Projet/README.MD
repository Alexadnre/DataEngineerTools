# Welcome to Amazon Web Scraper ğŸ‘‹

![Version](https://img.shields.io/badge/version-1.0-blue.svg?cacheSeconds=2592000)

> Un projet permettant de scrapper les produits Amazon, en recherchant un mot-clÃ© ou un lien, pour suivre lâ€™Ã©volution des prix et dÃ©tecter les opportunitÃ©s.

## Sommaire

1. [ğŸ—ï¸ Architecture du projet](#-architecture-du-projet)
2. [âš™ï¸ Installation & Configuration](#âš™ï¸-installation--configuration)
   - [1ï¸âƒ£ PrÃ©requis](#1ï¸âƒ£-prÃ©requis)
   - [2ï¸âƒ£ Installation](#2ï¸âƒ£-installation)
3. [ğŸ–¥ï¸ Interface Utilisateur](#ğŸ–¥ï¸-interface-utilisateur)
   - [ğŸ“ 1. Recherche & Scrapping](#ğŸ“-1-recherche--scrapping)
   - [ğŸ“ 2. Consultation des donnÃ©es scrappÃ©es](#ğŸ“-2-consultation-des-donnÃ©es-scrappÃ©es)
4. [ğŸ” DÃ©tails Techniques](#ğŸ”-dÃ©tails-techniques)
   - [ğŸ“Œ Scraping Amazon](#ğŸ“Œ-scraping-amazon)
   - [ğŸ“Œ Variables d'environnement](#ğŸ“Œ-variables-denvironnement)
   - [ğŸ“Œ MongoDB & Replica Set](#ğŸ“Œ-mongodb--replica-set)
   - [ğŸ“Œ Transfert Mongo â†’ Elastic](#ğŸ“Œ-transfert-mongo--elastic)
5. [ğŸ“¡ API & RequÃªtes](#ğŸ“¡-api--requÃªtes)
6. [ğŸ› ï¸ Tests & DÃ©veloppement](#ğŸ› ï¸-tests--dÃ©veloppement)
7. [ğŸ¥ DÃ©mo de l'Application](#ğŸ¥-dÃ©mo-de-lapplication)
8. [ğŸš€ AmÃ©liorations Futures](#ğŸš€-amÃ©liorations-futures)


---


# ğŸ—ï¸ Architecture du projet

Le projet repose sur plusieurs composants interconnectÃ©s :

- **Flask (interface graphique)** : permet d'interagir avec l'application.
- **Scraping (scrapping_script.py)** : rÃ©cupÃ¨re les donnÃ©es depuis Amazon.
- **Mongo1** : stocke les produits scrappÃ©s.
- **Task-executer** : planifie le scrapping automatique.
- **Mongo-to-Elastic** : transfert les donnÃ©es de MongoDB vers Elasticsearch.
- **Elasticsearch-db** : moteur de recherche et d'analyse pour afficher les historiques de prix.
- **Kibana** : visualisation avancÃ©e des donnÃ©es.

### ğŸ“Œ SchÃ©ma global 
![](./interface/static/img/Diagramme.png)

---

# âš™ï¸ Installation & Configuration

### 1ï¸âƒ£ PrÃ©requis

- **Docker & Docker Compose**
- **Python 3.9+**


### 2ï¸âƒ£ Installation

Clonez le projet :

```sh
git clone https://github.com/Alexadnre/DataEngineerTools.git

```

DÃ©marrez l'application :

```
cd DataEngineerTools/6Evaluation/Projet
docker-compose up --build -d

```

---

# ğŸ–¥ï¸ Interface Utilisateur

L'interface graphique possÃ¨de **deux pages** :

### ğŸ“ 1. Recherche & Scrapping
- Recherche par **mot-clÃ©** ou **URL**.
- SÃ©lection d'un **dÃ©lai d'exÃ©cution automatique**.
- ExÃ©cution du scrapping via **task-executer**.

![](./interface/static/img/screen_track.png)


### ğŸ“ 2. Consultation des donnÃ©es scrappÃ©es
- Affichage des **produits stockÃ©s**.
- Graphique interactif de **lâ€™historique des prix**.
- Auto-complÃ©tion via **Elasticsearch**.


![](./interface/static/img/screen_suivi.png)

---

# ğŸ” DÃ©tails Techniques

### ğŸ“Œ Scraping Amazon

**ProblÃ©matique** : Amazon, Ã©tant un site avec de nombreuses protections anti-scraping, peut bloquer les tentatives de scraping, notamment en raison des vÃ©rifications basÃ©es sur les user-agents et des demandes trop frÃ©quentes.

**Solution** :
Nous avons mis en place une sÃ©rie de techniques pour contourner ces protections et effectuer le scraping de maniÃ¨re plus efficace et discrÃ¨te :

1. **Utilisation d'un User-Agent ObsolÃ¨te** :
   - AprÃ¨s plusieurs tests, nous avons dÃ©couvert quâ€™en utilisant un **User-Agent d'une vieille version d'Internet Explorer**, nous parvenions Ã  Ã©viter les blocages. Cette mÃ©thode a permis dâ€™obtenir un accÃ¨s plus stable aux pages, car Amazon ne semble pas bloquer ce type de requÃªtes aussi facilement.
   
2. **DÃ©lai AlÃ©atoire entre les RequÃªtes** :
   - Nous avons ajoutÃ© des **dÃ©lai alÃ©atoires** entre chaque requÃªte pour simuler un comportement plus naturel et Ã©viter de trop solliciter le serveur Amazon.
   
3. **Rotation des User-Agents** :
   - Pour augmenter la robustesse de la solution, nous avons mis en place une **rotation des User-Agents**. Nous avons utilisÃ© un ensemble de **User-Agents fonctionnels** trouvÃ©s durant nos tests pour Ã©viter que le mÃªme user-agent soit utilisÃ© de maniÃ¨re rÃ©pÃ©tÃ©e.

4. **Utilisation de Microsoft Edge pour l'Analyse HTML** :
   - Pour mieux comprendre la structure des pages dâ€™Amazon et savoir comment effectuer nos requÃªtes de scraping, nous avons utilisÃ© **Microsoft Edge** avec la fonctionnalitÃ© permettant de charger des pages au format **Internet Explorer**. Bien que cette approche soit **moins stable** et utilise beaucoup de mÃ©moire, elle nous a permis de mieux analyser l'HTML des pages et de dÃ©finir les meilleures requÃªtes pour extraire les informations dont nous avons besoin.
   
Cette mÃ©thode nous a permis de contourner efficacement les protections tout en maintenant une certaine stabilitÃ© et fiabilitÃ© dans notre processus de scraping.


### ğŸ“Œ Variables d'environement 
Toutes les variables d'envirnnement sont stockÃ©es dans un  **.env**, pour simplifier l'utilisation des conteneurs 

```python
# Nom des containers
MONGO_PRIMARY_CONTAINER=mongo1
MONGO_INIT_CONTAINER=mongo-init
ELASTICSEARCH_CONTAINER=elasticsearch-db
KIBANA_CONTAINER=kibana
MONGO_TRACKER_CONTAINER=mongo-tracker-py


# MongoDB

MONGO_HOST=mongo1
MONGO_PORT=27017
MONGO_REPLICA_SET=rs0
MONGO_DB=amazon_web_scraper
MONGO_DB_TABLE=products
MONGO_DB_TABLE_TRACKER=track_list

# Elasticsearch
ELASTIC_HOST=elasticsearch
ELASTIC_PORT=9200

# Kibana
KIBANA_PORT=5601

# Flask
FLASK_APP=app.py
FLASK_ENV=development



TASK_EXECUTER_CONTAINER=task-executer
```
### ğŸ“Œ MongoDB & Replica Set
Les donnÃ©es sont stockÃ©es dans **MongoDB**, configurÃ© en **Replica Set** pour assurer la synchronisation et la durabilitÃ©.

Extrait du `docker-compose.yml` :

```yaml
services:
  mongo1:
    image: mongo:6
    container_name: ${MONGO_PRIMARY_CONTAINER}
    command: ["mongod", "--replSet", "${MONGO_REPLICA_SET}", "--bind_ip_all"]
    ports:
      - "${MONGO_PORT}:${MONGO_PORT}"
    volumes:
      - mongo-data:/data/db
    networks:
      - mongo-cluster
```

### ğŸ“Œ Transfert Mongo â†’ Elastic
Les produits sont transfÃ©rÃ©s vers **Elasticsearch** grÃ¢ce Ã  `mongo_to_elastic/tracker.py` en exploitant **MongoDB Replica Set**.

Extrait du code :

```python
pipeline = [
    {
        "$match": {
            "operationType": {"$in": ["insert", "update", "delete"]},  # Filtrer pour insert, update, delete
            "ns.coll": MONGO_DB_TABLE  # Filtrer pour la collection spÃ©cifique
        }
    }
]   

# Surveiller les changements et les envoyer Ã  Elasticsearch
with db.watch(pipeline) #...
```

---

# ğŸ“¡ API & RequÃªtes

Lâ€™interface utilise plusieurs **endpoints** :

| Endpoint                       | MÃ©thode | Description |
|---------------------------------|---------|-------------|
| `/scrape`                       | `POST`  | Lance le scrapping pour un mot-clÃ© ou une URL d'Amazon. |
| `/products`                     | `GET`   | RÃ©cupÃ¨re tous les produits stockÃ©s dans la base de donnÃ©es MongoDB. |
| `/history/<product_id>`         | `GET`   | Retourne l'historique des prix pour un produit spÃ©cifique. |
| `/track`                        | `GET`, `POST` | Permet de suivre l'ajout de produits et de lancer des processus de suivi des prix. |
| `/delete_product`               | `POST`  | Permet de supprimer un produit de la liste de suivi par son `asin`. |
| `/check_connections`            | `GET`   | VÃ©rifie la connexion Ã  MongoDB et Elasticsearch. |
| `/autocomplete`                 | `GET`   | Fournit une auto-complÃ©tion de produits en fonction de la requÃªte de recherche. |
| `/product/<product_id>`         | `GET`   | Retourne les dÃ©tails d'un produit Ã  partir de son `product_id` (ASIN). |

â¡ï¸ **Extrait JSON d'un produit scrappÃ©.**
```json
{'name': "NiPoGi AM06 Pro Mini PC 32 Go DDR5 RAM, ĞMD Ryzen 7 6800H(8C/16T, jusqu'Ã  4,7 GHz), 512Go M.2 SSD Mini Ordinateur de Burea...", 'price': 399.99, 'asin': 'B0DT3DN71D', 'url': 'https://www.amazon.fr/dp/B0DT3DN71D', 'image_url': 'https://m.media-amazon.com/images/I/71NuJFbTBCL._AC_UL320_.jpg'}
```
---

# ğŸ› ï¸ Tests & DÃ©veloppement

### Fichiers de test :

- `test/test_mongo_connection.py` : vÃ©rifie la connexion MongoDB.
- `test/test_mongo_request.py` : teste les requÃªtes Mongo.
- `test/test_update.py` : vÃ©rifie la mise Ã  jour des prix.

---


# ğŸ¥ DÃ©mo de l'Application

### Stockage et mise Ã  jour des donnÃ©es

Aucune donnÃ©e n'est stockÃ©e de base dans l'application. Toutes les informations sont rÃ©cupÃ©rÃ©es en temps rÃ©el, aprÃ¨s avoir suivi un produit. Le suivi des produits s'effectue de maniÃ¨re dynamique et les donnÃ©es sont mises Ã  jour directement aprÃ¨s chaque action de suivi.

Pour ajuster la frÃ©quence du suivi automatique, l'intervalle de mise Ã  jour peut Ãªtre configurÃ©. Par dÃ©faut, l'intervalle est dÃ©fini en heures, mais il est possible de le personnaliser pour effectuer des mises Ã  jour plus frÃ©quentes (par minute ou par seconde). 

Cela est nÃ©ant-moins dÃ©conseillÃ©, car augemnter la frÃ©quence des requettes augmente la chance qu'amazon bloque notre bot de scrapping. Cette section sert seulement Ã  vÃ©rifier le bon fonctionnement du code sans attendre plusieures heures

Pour cela, modifie le fichier `task-executer\task_executer.py`, **ligne 93** :

```python
interval = timedelta(hours=task["interval_hours"])  # /!\ intervale en heures
```
Il est ici possible de remplacer **hours=** par **minutes=** ou **seconds=** 
---
Voici maintenant une prÃ©sentation des diffÃ©rentes Ã©tapes d'utilisation de l'application, accompagnÃ©e de captures d'Ã©cran pour chaque Ã©tape clÃ©.

## 1ï¸âƒ£ **Page d'Accueil**

Lorsque vous accÃ©dez Ã  l'application, vous Ãªtes accueilli par la page d'accueil. Vous y trouverez les options pour commencer une recherche par mot-clÃ© ou par URL, ainsi qu'un bouton pour visualiser les produits suivis.


---

## 2ï¸âƒ£ **Recherche et Scrapping**

Dans cette section, vous pouvez entrer un mot-clÃ© ou une URL de produit Amazon pour commencer le processus de scrapping. Vous pouvez Ã©galement dÃ©finir un dÃ©lai d'exÃ©cution automatique pour le scrapping.

- **EntrÃ©e d'un mot-clÃ© ou d'une URL** :
  
![Recherche par mot-clÃ©](./interface/static/img/screen_home.png)

- **Configuration du dÃ©lai d'exÃ©cution automatique** :

![DÃ©lai d'exÃ©cution automatique](./interface/static/img/screen_delay.png)

---

## 3ï¸âƒ£ **ExÃ©cution du Scrapping**

Une fois la recherche lancÃ©e, l'application commence Ã  scrapper les donnÃ©es des produits Amazon. Voici un aperÃ§u de l'interface pendant l'exÃ©cution du scrapping.

![ExÃ©cution du scrapping](./interface/static/img/screen_scrapping.png)

---

## 4ï¸âƒ£ **Rechercher avec Auto-ComplÃ©tion**

GrÃ¢ce Ã  Elasticsearch, l'application vous permet de rechercher facilement des produits avec une fonction d'auto-complÃ©tion. Voici un aperÃ§u de la recherche avec auto-complÃ©tion en action.


![Liste des produits](./interface/static/img/screen_products.png)

---

## 5ï¸âƒ£ **Suivi des Produits**

AprÃ¨s l'exÃ©cution du scrapping, vous pouvez consulter la liste des produits scrappÃ©s. Vous pouvez voir des informations comme le prix, le nom du produit, et un graphique interactif montrant l'Ã©volution du prix dans le temps.


- **Historique des prix** :

![Historique des prix](./interface/static/img/screen_history.png)



---

## 6ï¸âƒ£ **Alertes sur les Baisses de Prix** (FonctionnalitÃ© Ã  venir)

Une fonctionnalitÃ© en dÃ©veloppement permet de recevoir des alertes lorsque les prix d'un produit baissent. Vous recevrez ces alertes par email ou SMS selon vos prÃ©fÃ©rences.


---

Ces Ã©tapes montrent comment l'application permet d'effectuer un scrapping de produits Amazon et de suivre l'Ã©volution de leurs prix. N'hÃ©sitez pas Ã  tester l'application et Ã  explorer ces diffÃ©rentes fonctionnalitÃ©s!


# ğŸš€ AmÃ©liorations Futures


### 1ï¸âƒ£ IntÃ©gration de Notifications pour Alerter sur les Baisses de Prix ğŸ“©
**Objectif** : Ajouter un systÃ¨me de notification pour alerter les utilisateurs des baisses de prix des produits suivis.

- **Approche** : 
  - ImplÃ©menter un systÃ¨me de tÃ¢ches planifiÃ©es via **Flask** qui vÃ©rifiera pÃ©riodiquement les prix des produits dans la base de donnÃ©es.
  - Lorsqu'une baisse de prix est dÃ©tectÃ©e, une notification sera envoyÃ©e via un service comme **Twilio** (SMS) ou **SendGrid** (email).
  
- **Technologies** :
  - Utilisation de **Flask-Mail** pour l'envoi d'emails ou **Flask-SocketIO** pour les notifications en temps rÃ©el sur l'interface utilisateur.
  - IntÃ©gration d'un service tiers pour les notifications par SMS (par exemple, Twilio).

### 2ï¸âƒ£ AmÃ©lioration du Moteur de Recherche via NLP ğŸ¤–
**Objectif** : AmÃ©liorer l'auto-complÃ©tion et la recherche en utilisant des techniques de **traitement du langage naturel (NLP)**.

- **Approche** : 
  - ComplÃ©ter le moteur de recherche **Elasticsearch** avec des modÃ¨les **NLP** pour mieux comprendre et traiter les requÃªtes des utilisateurs.
  - Utiliser des bibliothÃ¨ques telles que **spaCy** ou **Transformers** (par exemple, **BERT**) pour amÃ©liorer la pertinence des suggestions d'auto-complÃ©tion et des rÃ©sultats de recherche.
  
- **Technologies** :
  - IntÃ©gration de **spaCy** pour l'analyse linguistique et la catÃ©gorisation des termes de recherche.
  - Utilisation de modÃ¨les prÃ©-entrainÃ©s comme **BERT** ou **DistilBERT** pour l'analyse sÃ©mantique des requÃªtes.

### 3ï¸âƒ£ AmÃ©lioration de l'Interface Utilisateur ğŸŒ
**Objectif** : Refondre l'interface graphique pour amÃ©liorer l'expÃ©rience utilisateur.

- **Approche** : 
  - Ajouter des Ã©lÃ©ments d'interface tels que des graphiques en temps rÃ©el des baisses de prix, une meilleure prÃ©sentation des historiques de prix, et des alertes visuelles.
  - Introduire des **animations CSS** pour une expÃ©rience plus fluide et interactive.
  - Ajouter des filtres avancÃ©s pour trier les produits par catÃ©gorie, prix, ou score moyen des avis.
  
- **Technologies** :
  - Utilisation de **JavaScript** et **React.js** pour une interface plus dynamique.
  - Ajouter un graphique interactif avec **Plotly** ou **D3.js** pour visualiser l'Ã©volution des prix.
  
### 4ï¸âƒ£ FonctionnalitÃ© de Suppression de Produit sur l'Interface ğŸ¯
**Objectif** : Permettre Ã  l'utilisateur de supprimer facilement un produit de la liste de suivi via l'interface graphique.

- **Approche** : 
  - Ajouter un bouton "Supprimer" Ã  cÃ´tÃ© de chaque produit dans la liste de suivi.
  - Lorsque l'utilisateur clique sur "Supprimer", la base de donnÃ©es sera mise Ã  jour pour retirer le produit, et l'interface sera rÃ©actualisÃ©e.
  
- **Technologies** :
  - Utilisation de **JavaScript** pour la mise Ã  jour dynamique de l'interface.
  - IntÃ©gration de l'API backend pour supprimer les produits dans la base de donnÃ©es MongoDB.

### 5ï¸âƒ£ AmÃ©lioration de l'API pour Supporter plus de Formats de DonnÃ©es ğŸ”„
**Objectif** : Ã‰tendre l'API pour accepter et renvoyer des donnÃ©es dans plus de formats, tels que CSV ou XML.

- **Approche** : 
  - Ajouter des options dans les requÃªtes API pour exporter les produits et leurs historiques sous forme de **fichiers CSV** ou **XML**.
  - ImplÃ©menter des routes API supplÃ©mentaires pour la conversion et l'exportation de donnÃ©es.
  
- **Technologies** :
  - Utilisation de la bibliothÃ¨que **pandas** pour gÃ©nÃ©rer les fichiers CSV.
  - Utilisation de **xml.etree.ElementTree** pour gÃ©rer les exportations au format XML.

### 6ï¸âƒ£ Exemple de Scraping HÃ©bergÃ© sur AWS

Une version web du projet devait initialement Ãªtre hÃ©bergÃ©e sur **AWS** afin de fournir un exemple de scraping en continu. Cette version aurait permis de dÃ©montrer l'efficacitÃ© du scraping sur une longue pÃ©riode et de montrer les donnÃ©es en temps rÃ©el. Cependant, nous avons rencontrÃ© plusieurs dÃ©fis techniques :

- **ProblÃ¨me avec Elasticsearch** : L'instance AWS **t2 du tiers gratuit** Ã©tait insuffisante pour supporter la charge d'**Elasticsearch**, qui est trÃ¨s gourmand en ressources. MÃªme aprÃ¨s avoir mis Ã  niveau l'instance vers une machine plus puissante et configurÃ© les ports correctement, nous n'avons pas rÃ©ussi Ã  charger **Flask** correctement, en raison d'un problÃ¨me d'ouverture des ports sur AWS.
  
- **Solution Ã  Long Terme** : Ce problÃ¨me peut Ãªtre rÃ©solu avec **plus de temps** pour ajuster la configuration du serveur, optimiser les performances d'Elasticsearch et tester diffÃ©rentes configurations de ressources.

---



# âœ¨ Auteurs

ğŸ‘¤ **Alexandre Videlaine & Parys Noyon**

- Github: [@Alexadnre](https://github.com/Alexadnre)
- Github: [@Parysnm](https://github.com/Parysnm)
